---
title: "Homework 3"
author: "Rohan Sadale"
output: pdf_document
---


```{r message=FALSE}
library(alr4)
```

### 2.6

+ 2.6.1
```{r}
summary(ftcollinstemp)
plot(winter~fall, data = ftcollinstemp)
```

There is a very small linear trend in the plot. Most of the values are scattered especially when temperature in Fall becomes greater than 47 F. However most of time the temp in winter lies around 30 F.

+ 2.6.2

```{r}
plot(winter~fall, data = ftcollinstemp)
model1 <- lm(winter~fall, data = ftcollinstemp)
abline(model1)
#Tesing null hypothesis
summary(model1)
t <- (0.3132 - 0) / 0.1528
p_values <- 2*pt(-t, 109)
print( p_values)
```

As we can see p-value is less than 0.05. Thus we can reject NULL hypothesis.

+ 2.6.3

```{r}
summary(model1)
```

From the summary, we can see that R-squared value is 0.0371. This shows that about 3% of variability in the observed values of Winter temp can be explained by the fall temp.

+ 2.6.4

```{r}
data_early = ftcollinstemp[ftcollinstemp$year<1990,]
data_later = ftcollinstemp[ftcollinstemp$year>=1990,]

plot(data_early$winter ~ data_early$fall)
model2 <- lm(data_early$winter ~ data_early$fall)
abline(model2)

plot(data_later$winter ~ data_later$fall)
model3 <- lm(data_later$winter ~ data_later$fall)
abline(model3)

summary(model2)
summary(model3)
```

In case of model2(year < 1990), we have df = 88. Whereas in case of model3(Year > 1990) df = 19. There are very less data points in model3 as compared to model2. The R-squared value of model2 is better than model3 which can suggest that in model2 winter temp can be better explained by fall temp as compared to model3. However, in both models p-values are greater than 0.05, thus we fail to reject the null hypothesis i.e. accept null hypothesis.


### 2.16


+ 2.16.1

```{r}
model4 <- lm(log(fertility) ~ log(ppgdp), UN11) 
print(model4)
```


+ 2.16.2

```{r}
plot(log(fertility) ~ log(ppgdp), UN11) 
abline(model4)
```

+ 2.16.3

```{r}
summary(model4)
t <- (-0.20715-0)/0.01401
p_values <- pt(-abs(t), 197)
```

As p-value is less than 0.05, we reject the null hypothesis

+ 2.16.4

```{r}
summary(model4)
```

Coefficient of Determination is 0.526
This states that with 52.6% of variability in observed values of log(fertility) can be explained by log(ppgdp)

+ 2.16.5

```{r}
predict <- predict(model4, data.frame(ppgdp=1000),interval="prediction",level=0.95)
print(c(exp(predict[2]), exp(predict[3])))
```

+ 2.16.6
```{r}
print(UN11[UN11$fertility == max(UN11$fertility),][1])
print(UN11[UN11$fertility == min(UN11$fertility),][1])

# Two Largest Negative
print(sort(residuals(model4))[1])
print(sort(residuals(model4))[2])
# Two Largest Positive
print(sort(residuals(model4))[198])
print(sort(residuals(model4))[199])

```


### 2.20

+ 2.20.1
```{r}
plot(Interval ~ Duration, oldfaith)
model5 <- lm(Interval ~ Duration, oldfaith)
abline(model5)
print(coefficients(model5))
```

The plot shows that the Interval(time to next eruption) increases linearly with the Duration of the current eruption. The slope states that the for 1 sec increase in duration the time to next eruption increases by 0.17 sec

+ 2.20.2

```{r}
predict <- predict(model5, data.frame(Duration=250),interval="prediction",level=0.95)
print(predict)
```

+ 2.20.3

```{r}
predict <- predict(model5, data.frame(Duration=250),interval="prediction",level=0.90)
print(predict)[3]
```


### 3.2

+ 3.2.1

```{r}
m <- data.frame(fertility = UN11$fertility, logppgdp = log(UN11$ppgdp), pctUrban = UN11$pctUrban)
plot(m)
```

From the scatter plot we can see that log(ppgdp) and pctUrban are strongly correlated with each other, and there is a plausibility for a good fit of a simple linear regression model. We can also see that fertility is more correlated to log(ppgdp) than to pctUrban.

+ 3.2.2

```{r}
model6 <- lm(UN11$fertility ~ log(UN11$ppgdp))
print(model6) 

model7 <- lm(UN11$fertility ~ (UN11$pctUrban))
print(model7) 
summary(model7)
```

We can see that the slope coefficients are significantly different from 0. This is because p-value is too low and we reject NULL hypotesis.


+ 3.2.3

```{r}
model8 <- lm(fertility~pctUrban, data=UN11)
model9 <- lm(log(ppgdp)~pctUrban, data = UN11)
model10 <- lm(residuals(model8) ~ residuals(model9))
#summary(model8)
#summary(model9)
summary(model10)
```

Looking at the R-squared value, we can say that log(ppgdp) explains 31.93% of remaining variability in fertility after adjusting for pctUrban. Thus log(ppgdp) is useful after adjusting for pctUrban.

```{r}
model11 <- lm(fertility~log(ppgdp), data=UN11)
model12 <- lm(pctUrban~log(ppgdp), data = UN11)
model13 <- lm(residuals(model11) ~ residuals(model12))
#summary(model11)
#summary(model12)
summary(model13)
```

As R-squared value is very small, pctUrban is not useful for explaining remaining variability in fertility after adjusting for log(ppgdp).

```{r}
model14 = lm(fertility~log(ppgdp)+pctUrban, data=UN11)
summary(model14)
```

From the summary(model14) we can see that slope for log(ppgdp) and pctUrban is same as what we obtained from models - model10(log(ppgdp) after adjusting for pctUrban) and model13 (pctUrban after adjusting for log(ppgdp)) respectively. We can also see the coefficient of determination of model14 is similar to coefficient of model developed by fertility vs log(ppgdp). This suggests that addition of pctUrban variable to the regression isn't useful.


+ 3.2.4

```{r}
coefficients(model14)
coefficients(model10)
```

From above we can say that estimated coefficient for log(ppgdp) is the same as the estimated slope in the added-variable plot for log(ppgdp) after pctUrban.


+ 3.2.5

```{r}
isTRUE(all.equal(residuals(model14), residuals(model10)))
isTRUE(all.equal(residuals(model14), residuals(model13)))
```


+ 3.2.6

```{r}
summary(model14)
summary(model10)
```

From the summary we can see that t-value for the coefficient for log(ppgdp) is not quite the same from the added-variable plot and from the regression with both regressors. The reason is because difference in degrees of freedom. In case of model with two regressors, we are taking 3 degrees of freedom out(two for prediction and one for response)(df = 199-3 = 196). On the other hand, in case of model with one regressor, we are taking 2 degrees of freedom out (199-2 = 197). 
